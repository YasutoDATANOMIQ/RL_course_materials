# This code is from
# 布留川英一 著「AlphaZero 深層学習・強化学習・探索 人工知能プログラミング 実践入門」, 2019, ボーンデジタル
# Eiichi Hurukawa, "AlphaZero Deep Learning・Reinforcement Learning・Searching Artificial Intelligence Programming Practical Introduction", 2019, Bone Digital
# Its code is available here: https://book.borndigital.jp/support/AlphaZero/sample.zip
# I modified it only a bit.
# I would like to make a visualization of the process of the agent discovering better slot machines in the future.
import numpy as np
import random
import math
import pandas as pd
import matplotlib.pyplot as plt

# Making a class of each slot
class SlotArm():
    def __init__(self, p, r):
        self.p = p  # p is the probability of getting a reward from the slot machine.
        self.r = r  # r is the value of a reward from the slot machine.

    def draw(
            self):  # draw() of each slot machine gives out a reward of r with probability of p, otherwise reward of 0.0
        if self.p > random.random():
            return self.r
        else:
            return 0.0


# A class ofε-greedy algorithm.
# Or rather I should say this is class implementing behaviors of an agent, I mean a bandit.
class EpsilonGreedy():
    # self.epsilon is the value of ε, a probability of taking an action which is not thought to be best.
    def __init__(self, epsilon=0.1):
        self.epsilon = epsilon

    def initialize(self, n_arms):
        self.n = np.zeros(n_arms)  # An arary to store how many times each slot machine was selected.
        self.v = np.zeros(n_arms)  # An array to store values of each slot machine.
        # The higher the value is, the more likely the slot machine is selected.

    # A function for selecting a slot machine.
    def select_arm(self):
        # With probability of ε, the agent
        if self.epsilon > random.random():
            return np.random.randint(0, len(self.v))
        else:
            return np.argmax(self.v)

    # Updating parameters
    def update(self, chosen_arm, reward, t):
        # Counting how many times each slot machine has been chosen so far .
        self.n[chosen_arm] += 1

        # Updating the vlaues of the chosen slot machine.
        n = self.n[chosen_arm]
        v = self.v[chosen_arm]
        self.v[chosen_arm] = ((n - 1) / float(n)) * v + (1 / float(n)) * reward

    def label(self):
        return 'ε-greedy(' + str(self.epsilon) + ')'


# Executing all the simulations.
def play(algo, arms, num_sims, num_time):  # The agent runs slot machines num_time times in total in one simualtion.
    # The agent does simulation num_sims times.

    # Preparing arrays to store time steps and the reward at each time step.
    times = np.zeros(num_sims * num_time)
    rewards = np.zeros(num_sims * num_time)

    for sim in range(num_sims):
        algo.initialize(len(arms))

        for time in range(num_time):
            index = sim * num_time + time

            times[index] = time + 1
            chosen_arm = algo.select_arm()
            reward = arms[chosen_arm].draw()
            rewards[index] = reward

            # Updating parameters.
            algo.update(chosen_arm, reward, time + 1)

    # Returning [At which iteration, reward]
    return [times, rewards]


if __name__ == '__main__':

    # Preparing two type of slot machine arrangements, which I call "casinos."
    # In the first type, every slot machine gives out equal reward. Thus the bandit just has to find the slot machine which gives out coins the most likely.
    # In the second type, at slot machines with low odds give out higher rewards, so the angent are not supposed to
    # merely finding the slot machine with high odds.

    casino_A = (SlotArm(0.1, 1), SlotArm(0.3, 1), SlotArm(0.5, 1), SlotArm(0.9, 1))
    casino_B = (SlotArm(0.1, 10), SlotArm(0.3, 8), SlotArm(0.5, 3), SlotArm(0.9, 1))

    # The following 'algos' is a tuple of "bandits."
    # Each parameter of 'epsilon' means how likely the bandit tries a slot which is not thought to be the best.
    # In other words, the higher the 'epsilon' is, the more curious the bandit is.
    algos = (
    EpsilonGreedy(epsilon=0.1), EpsilonGreedy(epsilon=0.3), EpsilonGreedy(epsilon=0.5), EpsilonGreedy(epsilon=0.8))

    plt.figure(figsize=(20, 8))

    for algo in algos:
        results = play(algo, casino_A, num_sims=1000, num_time=250)
        df = pd.DataFrame({'times': results[0], 'rewards': results[1]})
        mean = df['rewards'].groupby(df['times']).mean()
        plt.subplot(1, 2, 1)
        plt.plot(mean, label=algo.label())
        plt.title("Casino A", fontsize=30)
        plt.xlabel('Step', fontsize=20)
        plt.ylabel('Average Reward', fontsize=20)
        plt.legend(loc='best')

    for algo in algos:
        results = play(algo, casino_B, num_sims=1000, num_time=250)
        df = pd.DataFrame({'times': results[0], 'rewards': results[1]})
        mean = df['rewards'].groupby(df['times']).mean()
        plt.subplot(1, 2, 2)
        plt.plot(mean, label=algo.label())
        plt.title("Casino B", fontsize=30)
        plt.xlabel('Step', fontsize=20)
        plt.ylabel('Average Reward', fontsize=20)
        plt.legend(loc='best')
    plt.savefig("multi_armed_bandit_logs.png")
    plt.show()